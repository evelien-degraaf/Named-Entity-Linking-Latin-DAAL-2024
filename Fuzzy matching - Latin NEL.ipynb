{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f37a9ba7-9231-4a63-bebd-ad1cbcda01c0",
   "metadata": {},
   "source": [
    "# Fuzzy string matching Latin personal names\n",
    "\n",
    "This notebook is intended as an explanation and illustration of the use of the [RapidFuzz library](https://maxbachmann.github.io/RapidFuzz/index.html) for matching Latin personal names in texts to potential surface forms in a name dictionary. The name dictionary contains name + potential candidate ID's, as is illustrated in the table below.\n",
    "|Key (name) |Value (mapping entity) |\n",
    "--- | --- |\n",
    "|Iulius|Iulius 1 (RE-Iulius_1)|\n",
    "| | ... |\n",
    "| | Iulius 16 (RE-Iulius_16) etc.|\n",
    "|C. Iulius|Iulius 16 (RE-Iulius_16)|\n",
    "|Gaius Iulius|Iulius 16 (RE-Iulius_16)|\n",
    "\n",
    "The aim is to match any surface form in text to all its potential IDs in _Paulys RealencyclopÃ¤die der classischen Altertumswissenschaft_ (_RE_). The section **Employing RapidFuzz to match** contains the exact code used to match the entities from the gold standard, including a detailed explanation of the multi-token matching."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67056327-1416-40d2-93f3-37c93a769ab5",
   "metadata": {},
   "source": [
    "## Installing and importing Rapidfuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad6f041-1e85-426e-9839-b434b71f7dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#if Rapidfuzz is not installed, it will be installed:\n",
    "import pkg_resources\n",
    "\n",
    "REQUIRED_PACKAGES = [\n",
    "    'rapidfuzz' \n",
    "]\n",
    "\n",
    "for package in REQUIRED_PACKAGES:\n",
    "    try:\n",
    "        dist = pkg_resources.get_distribution(package)\n",
    "        print('{} ({}) is installed'.format(dist.key, dist.version))\n",
    "    except pkg_resources.DistributionNotFound:\n",
    "        print('{} is NOT installed'.format(package))\n",
    "        !pip install {package}\n",
    "\n",
    "#import relevant functions\n",
    "\n",
    "from rapidfuzz import process, fuzz, utils\n",
    "from rapidfuzz.process import extractOne, extract\n",
    "from rapidfuzz.fuzz import ratio, token_ratio\n",
    "from rapidfuzz.distance import Indel, Levenshtein\n",
    "import pandas as pd\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078dd2e1-1e83-4de3-b322-7ca49c1dde3d",
   "metadata": {},
   "source": [
    "## Testing performance of different types of matching\n",
    "Many different options exist for Fuzzy matching based on the type of distance used and the threshold employed.\n",
    "\n",
    "First, a comparison between partial and normal ratio is offered to demonstrate the complications when using partial matching. Second, two different type of distance measures are compared Indel (standard in RapidFuzz) and Levenshtein (also a common type of distance). Other distances such as Jaro-Winkler are not considered at this moment but could be employed if desired."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70df404-8d5c-4576-8548-aef3f096fa6d",
   "metadata": {},
   "source": [
    "### Partial vs. Normal ratio\n",
    "\n",
    "The surface form **seruius galba** should match with the potential surface form in the name dictionary (e.g. **Seruius Sulpicius Galba**). Using partial_token_ratio will match 100 with any token that contains the substring and will thus match a 100 percent with any name that contains either seruius or galba. The same is true for the token_ratio function. This is shown below. An issue is encountered when a longer string contains a complete other string as substring such as is the case with **Uespasianus** and **Asia**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a87d26-1fa5-4149-89a2-8b5522b1680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercase the strings\n",
    "str1 = \"seruius galba\".lower()\n",
    "str2 = \"Seruius Sulpicius Galba\".lower()\n",
    "str3= \"Seruius\".lower()\n",
    "str4 = \"Galba\".lower()\n",
    "str5 = \"asia\".lower()\n",
    "str6 = \"Uespasianus\".lower()\n",
    "\n",
    "# Calculate the partial ratio\n",
    "score = fuzz.partial_token_ratio(str1, str3)\n",
    "\n",
    "print(f'We want the surface form \"seruius galba\" to match to (1) \"Seruius Sulpicius Galba\", (2) \"Seruius\" and (3) \"Galba\". \\nPartial_token_ratio gives \\n {fuzz.partial_token_ratio(str1, str2)} for (1) \\n {fuzz.partial_token_ratio(str1, str3)} for (2) \\n {fuzz.partial_token_ratio(str1, str4)} for (3) \\n\\nToken_ratio gives \\n {fuzz.token_ratio(str1, str2)} for (1) \\n {fuzz.token_ratio(str1, str3)} for (2) \\n {fuzz.token_ratio(str1, str4)} for (3) \\n\\nWhen matching the string \"uespasianus\", the issue of partial matching becomes clear \\n Partial score uespasianus - Asia: {fuzz.partial_token_ratio(str5, str6)} \\n Normal ratio score uespasianus - Asia: {fuzz.token_ratio(str5, str6)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4277ab19-5acf-4a1e-8c84-0a3e0e5070a1",
   "metadata": {},
   "source": [
    "Partial matching will thus not suffice for matching as it will cause noise in the result. Therefore, the normal Token_ratio is preferred. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250b9140-6833-4380-8ef3-a4d21ffc17a6",
   "metadata": {},
   "source": [
    "### Indel or Levenshtein distance\n",
    "The main difference between these two distances is their approach to substitution. Compare what happens in cases of insertion and substitution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdf4ef7-85cd-47bc-8c33-95f42d55fb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"Saeuinus\".lower() #insertion\n",
    "str2 = \"Saeuinius\".lower()\n",
    "str3 = \"Uologaesus\".lower() #substitution\n",
    "str4 = \"Uologaeses\".lower()\n",
    "\n",
    "print(f'The standard RapidFuzz token_ratio relies upon Indel distance and gives the following scores for insertion (+1) and substitution (e ipv u): \\n Insertion: {fuzz.token_ratio(str1, str2) } \\n Substitution {fuzz.token_ratio(str3, str4)} \\n\\nCompare this to the Levenshtein similarity: \\n Insertion: {Levenshtein.normalized_similarity(str1, str2)} \\n Substitution {Levenshtein.normalized_similarity(str3, str4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbbe60a-89c9-4c27-9eb0-7f72c661c2e5",
   "metadata": {},
   "source": [
    "This shows that for Indel distance, substitution is considered a more complicated change than insertion whereas Levenshtein distance is the opposite, which is explained in the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df60a1b-7138-41b9-b319-e801bae8e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Indel explanation of getting from str1 to 2 and from 3 to 4\n",
    "print(f'Indel distance between the strings is calculated based on the following information: \\n Insertion: {Indel.editops(str1, str2)} \\n Substitution: {Indel.editops(str3, str4)}\\n')\n",
    "print(f'Levenshtein distance between the strings is calculated based on the following information: \\n Insertion: {Levenshtein.editops(str1, str2)} \\n Substitution: {Levenshtein.editops(str3, str4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18345f8f-ee27-43ba-b5a5-fd699f82562a",
   "metadata": {},
   "source": [
    "This shows that the substitution in Indel distance actually exists of two edits, an insert and a deletion, whereas for Levenshtein it is simply one edit, replace. Both functions are normalized, which mean they also take into account the length of the string (score = distance / (len1 + len2)). For our article, we chose to work with Indel distance as it is the standard in the Rapidfuzz library."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250f8008-d272-49b2-8138-ad85abf6ee19",
   "metadata": {},
   "source": [
    "## Rapidfuzz Extract\n",
    "The extract process provided by RapidFuzz allows for matching a string to a list of other strings, extracting each potential match. It allows you to specify the following parameters: \n",
    "- search string (query): in our case the **lemma** offered for a token or the **combination of lemmas** for multi-token entities (when lemma is not available, token is used)\n",
    "- choices: the list of names to match the search string to, which is provided by the name dictionary keys\n",
    "- scorer: define a scorer either provided by RapidFuzz or self-created for edit distance; we selected token_ratio\n",
    "- processor: we used the \"default_process\" trims whitespaces, ignores numbers and lowercases the strings\n",
    "- score_cutoff: any matches with a score below this score will not be returned; we selected a cutoff of 88\n",
    "- score_hint: optional argument for an expected score to be passed to the scorer; no expected score is specified\n",
    "- limit: maximum number of candidates to be generated (The list is sorted by the similarity. When multiple choices have the same similarity, they are sorted by their index); no limit is specified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b84a593-c699-4b5e-9123-a6496e91db94",
   "metadata": {},
   "source": [
    "### Cutoff explained\n",
    "The cutoff score was determined based on extensive experimentation, finding a balance between including too much noise and still matching often recurring differences. Two often occurring differences are:\n",
    "- No lemma is provided for the token and the thus the token itself is used for matching. These are often in a different case than the surface forms recorded in the name dictionary. For example, the name dictionary does not contain the forms _Hannibaliano_, just _Hannibalianus_. It is important that these still match if that is the only difference between the tokens. The example below illustrates that placing the cutoff at 88 will allow longer strings to be matched in this way, but shorter strings do not. However, placing the cutoff lower than 88, will introduce more noise in the data.\n",
    "- The lemma provided in the text has a different ending than the _RE_ entry: for example, the lemma recorded for _Laco_ in the name _Cornelius Laco_ is _Lacon_. The example below illustrates that placing the cutoff at 88 will allow this type of difference still to be matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10885bc-f054-4093-b5ab-2c00bdbb56ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "str1 = \"Hannibaliano\".lower() #insertion + deletion long string\n",
    "str2 = \"Hannibalianus\".lower()\n",
    "\n",
    "str3 = \"Iulio\".lower() #insertion + deletion short string\n",
    "str4 = \"Iulius\".lower()\n",
    "\n",
    "str5 = \"Lacon\".lower() #deletion + short string\n",
    "str6 = \"Laco\".lower()\n",
    "\n",
    "print(f'The score for matching a token without lemma (cased) scores as follows: \\n Long string: {fuzz.token_ratio(str1, str2)} \\n Short string: {fuzz.token_ratio(str3, str4)} \\n\\nThe score for matching a lemma with a different ending than the Latinized spelling in the RE: \\n Score: {fuzz.token_ratio(str5, str6)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc5105a-8d73-4e1f-b461-27ed10a0cc77",
   "metadata": {},
   "source": [
    "# Employing RapidFuzz to match\n",
    "The following code can be used to match any dictionary with token_ids as keys and names as values to the name dictionary that is available on the GitHub. In addition, the code used to exact match multi-token entities, i.e. when all sub-parts of the multi-token exist for the predicted IDs. For this, the ID dictionary is used that contains for each _RE_-ID all potential surface forms. For multi-token entities, the token_id used in the dictionary, is the id of the first token of the entity (the B- labelled entity).\n",
    "\n",
    "Below the process is illustrated with the sample sentence from Tacitus _Historiae_ 1 (sent_id = TacHist1-Q-05-26):\n",
    ">_tardum Galbae iter et cruentum interfectis Cingonio Uarrone consule designato et Petronio Turpiliano consulari ille ut Nymphidii socius hic ut dux Neronis inauditi atque indefensi tamquam innocentes perierant_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5371d2c-1a67-49d7-82f0-65f8b43b4adc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import name dictionary\n",
    "with open('./Real_name_dict_Latin.json', 'r') as json_file:\n",
    "    name_dict = json.load(json_file)   \n",
    "\n",
    "#import id dictionary\n",
    "with open('./Real_id_dict_Latin.json', 'r') as json_file:\n",
    "    id_dict = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35bf614-3ff2-4672-ac01-d391b6c2ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#establish a dictionary with token_ids as key and the strings to match as value\n",
    "test_dictionary = {\n",
    "    \"TokenURI=http://lila-erc.eu/data/corpora/Lasla/id/corpus/TacitusTac%20Historiae/Tacitus_TacHistoriae_TacHist1.BPN_t_0000689\": \"galba\",\n",
    "    \"TokenURI=http://lila-erc.eu/data/corpora/Lasla/id/corpus/TacitusTac%20Historiae/Tacitus_TacHistoriae_TacHist1.BPN_t_0000694\": \"cingonius uarro\", #multi-token entity uses the token_id of the first token as key\n",
    "    \"TokenURI=http://lila-erc.eu/data/corpora/Lasla/id/corpus/TacitusTac%20Historiae/Tacitus_TacHistoriae_TacHist1.BPN_t_0000699\": \"petronius turpilianus\",\n",
    "    \"TokenURI=http://lila-erc.eu/data/corpora/Lasla/id/corpus/TacitusTac%20Historiae/Tacitus_TacHistoriae_TacHist1.BPN_t_0000704\": \"nymphidius\",\n",
    "    \"TokenURI=http://lila-erc.eu/data/corpora/Lasla/id/corpus/TacitusTac%20Historiae/Tacitus_TacHistoriae_TacHist1.BPN_t_0000709\": \"nero\"   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0352c9-c00a-40a6-ace0-e92e90f0b67f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a new dictionary to store the results\n",
    "matched_dict = {}\n",
    "\n",
    "# Iterate over each item in the original dictionary \n",
    "for key, value in test_dictionary.items():\n",
    "    #use RapidFuzz extract with token_ratio to match each surface form to all potential matches in the name dictionary that have a score higher than 88\n",
    "    results = extract(value, name_dict.keys(), scorer=token_ratio, processor=utils.default_process, score_cutoff = 88, limit = None) \n",
    "\n",
    "    #empty list and set to store the results\n",
    "    match_lemmas = []\n",
    "    ids = set()\n",
    "    \n",
    "    for lemma, score, ind in results:\n",
    "        #for each result, store the matched form in a list\n",
    "        match_lemma = lemma, score\n",
    "        match_lemmas.append(match_lemma)\n",
    "        #retreive all potential RE_ids from the name dictionary and store in a set to ensure unique values only (i.e. remove all duplicate predictions)\n",
    "        ids.update(name_dict[lemma])\n",
    "    #store the results in a dictionary with the token_id as key and a list containing the string to match, the potential surface form matches, and the unique list of potential ids\n",
    "    matched_dict[key] = [value, match_lemmas, ids]    \n",
    "\n",
    "#for further processing & printing store the token_id and the potential matching RE_ids in a dictionary\n",
    "simple_match_fuzz = {key: value[2] for key, value in matched_dict.items()}\n",
    "print(simple_match_fuzz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7548156b-c0e7-47a4-873f-75f65703d89b",
   "metadata": {},
   "source": [
    "## Multi-token entities\n",
    "The code below uses the predictions from the previous block and limits the predictions for multi-token entities to only those IDs that have an exact match as potential surface form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f537a18-7159-4b41-8583-04fc55372905",
   "metadata": {},
   "outputs": [],
   "source": [
    "#provide for multi-tokens only those ids that contain all sub-parts of the multi-token in a surface form\n",
    "matched_dict_multi = {}\n",
    "\n",
    "for key, value in matched_dict.items():\n",
    "    #iterate over dictionary containing token_id as key and a list containing the string to match, the potential surface form matches, and the unique list of potential ids and retrieve those containing more than one token\n",
    "    nam_parts = value[0].split()\n",
    "    if len(nam_parts) > 1:\n",
    "        pot_match = []\n",
    "        for re_id in value[2]:\n",
    "            #for each RE_id predicted in the previous step, check if the lower case of all parts of the multi-token (nam_parts) matches exact any of the potential surface forms (nam_comps)\n",
    "            nam_comps = id_dict[re_id]\n",
    "            if set([item.lower() for item in nam_parts]).issubset(set([item.lower() for item in nam_comps])):\n",
    "                  pot_match.append(re_id)\n",
    "        \n",
    "        #if such a match is found, store this new prediction in the new dictionary, if not, retain the original predictions\n",
    "        if len(set(pot_match)) != 0:\n",
    "            matched_dict_multi[key] = value[0], value[1], set(pot_match)\n",
    "        else:\n",
    "            matched_dict_multi[key] = value\n",
    "    #if the entity is not longer than one word, retain the original prediction\n",
    "    else:\n",
    "        matched_dict_multi[key] = value\n",
    "        \n",
    "#for further processing & printing store the token_id and the potential matching RE_ids in a dictionary\n",
    "simple_match_multi = {key: list(value[2]) for key, value in matched_dict_multi.items()}\n",
    "print(simple_match_multi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9b0f57-31d2-46d3-a835-4c6668e1c8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in matched_dict_multi.items():\n",
    "    org_length = len(simple_match_fuzz[key])\n",
    "    multi_length = len(simple_match_multi[key])\n",
    "    entity = value[0]\n",
    "    print(f\"The number of predictions for entity '%s': \\norignal: %d \\t multi-token: %d \\n\" %(entity, org_length, multi_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868942dd-260e-4f53-a12b-45ca6a34bef1",
   "metadata": {},
   "source": [
    "The case of _cingonius uarro_ can illustrate what happens. For the original predictions, the potential surface forms are retrieved and checked for an exact match in any of the potential surface forms. As you can see in the printed results of the code below, only **RE-Cingonius** has a surface form that contains an exact match for both _cingonius_ and _uarro_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f31b3c6-128d-4b5b-98a3-2bd81eafcad3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "org_preds = matched_dict['TokenURI=http://lila-erc.eu/data/corpora/Lasla/id/corpus/TacitusTac%20Historiae/Tacitus_TacHistoriae_TacHist1.BPN_t_0000694'][2]\n",
    "nam_parts = matched_dict['TokenURI=http://lila-erc.eu/data/corpora/Lasla/id/corpus/TacitusTac%20Historiae/Tacitus_TacHistoriae_TacHist1.BPN_t_0000694'][0].split()\n",
    "\n",
    "for re_id in org_preds:\n",
    "    nam_comps = id_dict[re_id]\n",
    "    print(f\"For %s potential surface forms are %s\"%(re_id, nam_comps))\n",
    "    if set([item.lower() for item in nam_parts]).issubset(set([item.lower() for item in nam_comps])):\n",
    "        print(f\"MATCH: The sub-parts all (%s) have an exact match in a surface form of %s\\n\"%(nam_parts, re_id))\n",
    "    else:\n",
    "        print(f\"At least one of the sub-parts (%s) does not have an exact match in a surface form of %s\\n\"%(nam_parts, re_id))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
